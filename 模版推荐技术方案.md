# 模版生图推荐系统技术设计文档 (Golang + Python混合架构)

## 1. 需求分析

### 1.1 业务场景
用户通过对话框输入生图需求(如"帮我生成一张科技感的海报"),系统需要从已有模版库中智能匹配并推荐最符合用户意图的几个模版供用户选择。

### 1.2 核心需求
- 理解用户自然语言描述的生图需求
- 从模版库中检索最相关的模版
- 按相关度排序并推荐Top-K个模版
- 支持多轮对话,精确用户需求

---

## 2. Agent方案可行性分析

### 2.1 Agent方案优势

✅ **强大的意图理解能力**
- LLM可以准确理解用户的复杂、模糊的自然语言描述
- 支持上下文理解,处理多轮对话
- 能够提取关键特征:风格、场景、色调、用途等

✅ **灵活的工具调用**
- 可以调用向量检索工具进行语义匹配
- 可以调用标签过滤工具进行精准筛选
- 可以组合多个工具实现复杂检索逻辑

✅ **智能推理能力**
- 能够根据检索结果进行二次排序和筛选
- 可以解释推荐理由,提升用户体验
- 支持个性化推荐策略

### 2.2 Agent方案挑战

⚠️ **响应延迟**
- LLM推理需要时间(通常200-2000ms)
- 工具调用增加额外延迟
- 需要优化用户体验

⚠️ **成本考虑**
- 每次请求都需要调用LLM API
- 高并发场景下成本较高
- 需要考虑缓存和降级策略

⚠️ **稳定性**
- LLM输出存在不确定性
- 需要严格的工具调用格式验证
- 需要完善的错误处理机制

### 2.3 结论

**Agent方案完全可行,且是推荐方案**,原因:
1. 用户需求表达多样化,传统关键词匹配难以满足
2. 模版推荐重在准确性而非极致性能
3. Agent的灵活性便于后续功能扩展
4. 可通过混合架构平衡性能和成本

---

## 3. 整体流程设计

### 3.1 核心流程图

```
用户输入
    ↓
┌─────────────────────┐
│  Golang API Gateway │
│  - 请求验证         │
│  - 限流鉴权         │
│  - 缓存查询         │
└─────────────────────┘
    ↓
┌─────────────────────┐
│  Python AI Service  │
│  - LLM Agent        │
│  - 意图理解         │
│  - 特征提取         │
└─────────────────────┘
    ↓
┌─────────────────────┐
│  工具调用决策       │
└─────────────────────┘
    ↓
    ├─→ [Golang向量检索服务] → 语义相似度匹配
    ├─→ [Golang标签过滤服务] → 精准属性筛选
    └─→ [混合检索] → 综合检索策略
    ↓
┌─────────────────────┐
│  Golang结果聚合     │
│  - 多路召回融合     │
│  - 相关度评分       │
│  - Top-K筛选        │
└─────────────────────┘
    ↓
┌─────────────────────┐
│  Python生成推荐理由 │
│  - LLM解释          │
│  - 生成展示文案     │
└─────────────────────┘
    ↓
返回推荐结果
```

### 3.2 详细执行流程

**Step 1: 用户输入处理 (Golang)**
```
输入: "我需要一张简约风格的产品发布会海报,主色调蓝色"
  ↓
Golang API Gateway:
  - 敏感词过滤
  - 长度检查
  - 格式化
  - 查询缓存
```

**Step 2: Agent 理解与规划 (Python)**
```
调用Python AI Service (gRPC/HTTP):
LLM分析输出:
{
  "intent": "生成海报",
  "style": "简约风格",
  "scenario": "产品发布会",
  "color_scheme": "蓝色主调",
  "search_strategy": "hybrid",
  "keywords": ["简约", "产品", "发布会", "蓝色", "商务"],
  "tags": ["简约", "商务", "蓝色"]
}
```

**Step 3: 工具调用 (Golang)**
```
Golang并行调用检索服务:
1. VectorSearch(embedding, top_k=20)
2. TagFilter(tags, top_k=20)
3. KeywordSearch(keywords, top_k=10)
```

**Step 4: 结果融合 (Golang)**
```
算法: RRF (Reciprocal Rank Fusion)
最终得分 = 0.5 * vector_score + 0.3 * tag_score + 0.2 * keyword_score
```

**Step 5: 生成推荐 (Python)**
```
调用Python LLM生成推荐文案:
"为您推荐以下3个模版:
1. [商务发布会海报] - 采用蓝色主调,简约设计,适合科技产品发布
2. [现代企业海报] - 极简风格,蓝白配色,专业商务感
3. [产品展示模版] - 清爽蓝调,突出产品特性"
```

---

## 4. 技术架构设计

### 4.1 系统架构图

```
┌─────────────────────────────────────────────────────┐
│                    前端层                            │
│  - Web界面 (React/Vue)                              │
│  - 移动端 (React Native/Flutter)                    │
└─────────────────────────────────────────────────────┘
                        ↓ HTTP/WebSocket
┌─────────────────────────────────────────────────────┐
│              Golang API Gateway                      │
│  - Gin/Echo框架                                     │
│  - 请求验证、限流、鉴权                              │
│  - 缓存层 (Redis)                                    │
│  - 负载均衡                                          │
└─────────────────────────────────────────────────────┘
            ↓                               ↓
┌──────────────────────┐      ┌──────────────────────┐
│  Golang业务服务层     │      │  Python AI服务层     │
│  ┌────────────────┐  │      │  ┌────────────────┐ │
│  │ 模版管理服务    │  │      │  │ LLM Agent      │ │
│  │ 用户管理服务    │  │      │  │ - LangChain    │ │
│  │ 检索服务       │  │      │  │ - 意图理解     │ │
│  │ 排序融合服务    │  │      │  │ - 工具调度     │ │
│  └────────────────┘  │      │  └────────────────┘ │
│                      │      │  ┌────────────────┐ │
│                      │      │  │ Embedding服务  │ │
│                      │      │  │ - 向量生成     │ │
│                      │      │  └────────────────┘ │
└──────────────────────┘      └──────────────────────┘
            ↓ gRPC/HTTP                    ↓
┌─────────────────────────────────────────────────────┐
│                  数据存储层                          │
│  ┌──────────────┐  ┌──────────────┐  ┌──────────┐ │
│  │ PostgreSQL   │  │ 向量数据库    │  │ Redis    │ │
│  │ - 模版元数据  │  │ - Qdrant     │  │ - 缓存   │ │
│  │ - 用户数据    │  │ - Milvus     │  │ - 会话   │ │
│  │ - 统计数据    │  │              │  │          │ │
│  └──────────────┘  └──────────────┘  └──────────┘ │
│  ┌──────────────┐  ┌──────────────┐                │
│  │ RabbitMQ     │  │ MinIO/S3     │                │
│  │ - 异步任务    │  │ - 模版文件    │                │
│  └──────────────┘  └──────────────┘                │
└─────────────────────────────────────────────────────┘
                        ↓
┌─────────────────────────────────────────────────────┐
│              外部服务层                              │
│  ┌──────────────┐  ┌──────────────┐                │
│  │ OpenAI API   │  │ Anthropic API│                │
│  │ (Claude API) │  │              │                │
│  └──────────────┘  └──────────────┘                │
└─────────────────────────────────────────────────────┘
```

### 4.2 服务职责划分

#### 4.2.1 Golang服务 (高性能、高并发)
- ✅ API Gateway - 请求路由、验证、限流
- ✅ 业务逻辑服务 - 模版CRUD、用户管理
- ✅ 检索服务 - 向量检索、标签过滤、关键词搜索
- ✅ 结果聚合服务 - 多路召回融合、排序
- ✅ 缓存管理 - Redis操作
- ✅ 数据库操作 - PostgreSQL CRUD
- ✅ 对象存储操作 - MinIO/S3文件管理

#### 4.2.2 Python服务 (AI专用)
- ✅ LLM Agent - 意图理解、工具调用决策
- ✅ Embedding生成 - 文本向量化
- ✅ 推荐文案生成 - LLM生成解释文本
- ✅ 模型管理 - 本地模型加载和推理

### 4.3 服务间通信

**gRPC协议 (推荐)**
```protobuf
// ai_service.proto
syntax = "proto3";

service AIService {
  // 意图理解和特征提取
  rpc UnderstandIntent(IntentRequest) returns (IntentResponse);
  
  // 生成Embedding
  rpc GenerateEmbedding(EmbeddingRequest) returns (EmbeddingResponse);
  
  // 生成推荐文案
  rpc GenerateExplanation(ExplanationRequest) returns (ExplanationResponse);
}

message IntentRequest {
  string query = 1;
  string user_id = 2;
  repeated string context = 3;
}

message IntentResponse {
  string intent = 1;
  map<string, string> features = 2;
  repeated string keywords = 3;
  repeated string tags = 4;
  string search_strategy = 5;
}

message EmbeddingRequest {
  string text = 1;
}

message EmbeddingResponse {
  repeated float embedding = 1;
  int32 dimension = 2;
}

message ExplanationRequest {
  string query = 1;
  repeated Template templates = 2;
}

message ExplanationResponse {
  string explanation = 1;
  repeated string reasons = 2;
}
```

---

## 5. 技术栈选型

### 5.1 核心技术栈

| 层次 | 技术选型 | 说明 |
|------|---------|------|
| **后端服务** | Golang 1.21+ | 高性能API服务 |
| **Web框架** | Gin / Echo / Fiber | RESTful API框架 |
| **AI服务** | Python 3.11+ (FastAPI) | LLM调用和Agent编排 |
| **Agent框架** | LangChain / LlamaIndex | Agent编排和工具管理 |
| **LLM** | OpenAI GPT-4 / Claude 3.5 Sonnet | 意图理解和工具调用 |
| **向量数据库** | Qdrant / Milvus | 语义检索 |
| **关系数据库** | PostgreSQL 15+ (pgvector) | 模版元数据存储 |
| **缓存** | Redis 7+ | 热点数据缓存 |
| **消息队列** | RabbitMQ / Kafka | 异步任务处理 |
| **RPC框架** | gRPC | Go-Python服务通信 |
| **ORM** | GORM (Go) / SQLAlchemy (Python) | 数据库ORM |
| **Embedding模型** | OpenAI text-embedding-3 / BGE | 文本向量化 |
| **对象存储** | MinIO / S3 / OSS | 模版文件存储 |
| **监控** | Prometheus + Grafana | 性能监控 |
| **日志** | Zap (Go) / Loguru (Python) | 结构化日志 |
| **链路追踪** | Jaeger / OpenTelemetry | 分布式追踪 |

### 5.2 Golang关键库

```go
// go.mod
module template-recommend

go 1.21

require (
    github.com/gin-gonic/gin v1.9.1           // Web框架
    github.com/go-redis/redis/v8 v8.11.5      // Redis客户端
    gorm.io/gorm v1.25.5                      // ORM
    gorm.io/driver/postgres v1.5.4            // PostgreSQL驱动
    google.golang.org/grpc v1.59.0            // gRPC
    google.golang.org/protobuf v1.31.0        // Protobuf
    github.com/qdrant/go-client v1.7.0        // Qdrant客户端
    github.com/minio/minio-go/v7 v7.0.63      // MinIO客户端
    github.com/streadway/amqp v1.1.0          // RabbitMQ
    go.uber.org/zap v1.26.0                   // 日志
    github.com/prometheus/client_golang v1.17.0 // Prometheus
    github.com/spf13/viper v1.17.0            // 配置管理
    golang.org/x/sync v0.5.0                  // 并发工具
)
```

### 5.3 Python关键库

```python
# requirements.txt
fastapi==0.104.1
uvicorn[standard]==0.24.0
langchain==0.1.0
openai==1.6.1
anthropic==0.8.1
qdrant-client==1.7.0
grpcio==1.60.0
grpcio-tools==1.60.0
pydantic==2.5.0
numpy==1.26.2
torch==2.1.1  # 本地模型推理
transformers==4.36.0  # Hugging Face模型
sentence-transformers==2.2.2  # BGE等embedding模型
redis==5.0.1
prometheus-client==0.19.0
```

---

## 6. 数据模型设计

### 6.1 PostgreSQL表结构

```sql
-- 模版表
CREATE TABLE templates (
    id BIGSERIAL PRIMARY KEY,
    template_id VARCHAR(64) UNIQUE NOT NULL,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    category VARCHAR(50),
    tags TEXT[], -- PostgreSQL数组
    style VARCHAR(50),
    color_scheme VARCHAR(50),
    use_case VARCHAR(100),
    thumbnail_url VARCHAR(512),
    preview_url VARCHAR(512),
    file_url VARCHAR(512),
    
    -- 向量ID (存储在Qdrant的ID)
    vector_id VARCHAR(64),
    
    -- 元数据
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    status VARCHAR(20) DEFAULT 'active',
    
    -- 统计数据
    view_count INTEGER DEFAULT 0,
    use_count INTEGER DEFAULT 0,
    rating DECIMAL(3,2),
    
    -- 索引
    INDEX idx_template_id (template_id),
    INDEX idx_category (category),
    INDEX idx_tags USING GIN (tags),
    INDEX idx_style (style),
    INDEX idx_status (status),
    INDEX idx_created_at (created_at)
);

-- 用户交互记录表
CREATE TABLE user_interactions (
    id BIGSERIAL PRIMARY KEY,
    user_id VARCHAR(64) NOT NULL,
    session_id VARCHAR(64),
    query TEXT NOT NULL,
    intent JSONB, -- 存储意图分析结果
    recommended_templates JSONB, -- 推荐结果
    selected_template_id VARCHAR(64),
    feedback VARCHAR(20), -- 'positive', 'negative', 'neutral'
    response_time_ms INTEGER,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_user_id (user_id),
    INDEX idx_session_id (session_id),
    INDEX idx_created_at (created_at)
);

-- 用户表
CREATE TABLE users (
    id BIGSERIAL PRIMARY KEY,
    user_id VARCHAR(64) UNIQUE NOT NULL,
    username VARCHAR(100),
    email VARCHAR(255),
    preferences JSONB, -- 用户偏好
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    
    INDEX idx_user_id (user_id)
);
```

---

## 7. 核心代码实现

### 7.1 Golang - API Gateway

```go
// cmd/api/main.go
package main

import (
    "github.com/gin-gonic/gin"
    "template-recommend/internal/config"
    "template-recommend/internal/handler"
    "template-recommend/internal/middleware"
    "template-recommend/internal/service"
)

func main() {
    // 加载配置
    cfg := config.Load()
    
    // 初始化服务
    services := service.InitServices(cfg)
    
    // 初始化路由
    r := gin.Default()
    
    // 中间件
    r.Use(middleware.CORS())
    r.Use(middleware.RateLimit())
    r.Use(middleware.Logger())
    r.Use(middleware.Recovery())
    
    // 健康检查
    r.GET("/health", func(c *gin.Context) {
        c.JSON(200, gin.H{"status": "ok"})
    })
    
    // API路由组
    v1 := r.Group("/api/v1")
    {
        // 推荐接口
        recommend := v1.Group("/recommend")
        {
            h := handler.NewRecommendHandler(services)
            recommend.POST("", h.Recommend)
            recommend.POST("/feedback", h.SubmitFeedback)
        }
        
        // 模版接口
        templates := v1.Group("/templates")
        {
            h := handler.NewTemplateHandler(services)
            templates.GET("", h.ListTemplates)
            templates.GET("/:id", h.GetTemplate)
            templates.POST("", h.CreateTemplate)
            templates.PUT("/:id", h.UpdateTemplate)
        }
    }
    
    // 启动服务
    r.Run(":8080")
}
```

```go
// internal/handler/recommend_handler.go
package handler

import (
    "context"
    "net/http"
    "time"
    
    "github.com/gin-gonic/gin"
    "template-recommend/internal/model"
    "template-recommend/internal/service"
)

type RecommendHandler struct {
    recommendSvc *service.RecommendService
    cacheSvc     *service.CacheService
}

type RecommendRequest struct {
    Query  string `json:"query" binding:"required,max=500"`
    UserID string `json:"user_id"`
    TopK   int    `json:"top_k" binding:"min=1,max=20"`
}

type RecommendResponse struct {
    Status          string              `json:"status"`
    Query           string              `json:"query"`
    Recommendations []model.Template    `json:"recommendations"`
    Explanation     string              `json:"explanation"`
    ResponseTimeMs  int64               `json:"response_time_ms"`
}

func (h *RecommendHandler) Recommend(c *gin.Context) {
    startTime := time.Now()
    
    var req RecommendRequest
    if err := c.ShouldBindJSON(&req); err != nil {
        c.JSON(http.StatusBadRequest, gin.H{"error": err.Error()})
        return
    }
    
    // 设置默认值
    if req.TopK == 0 {
        req.TopK = 5
    }
    
    ctx := context.Background()
    
    // 1. 检查缓存
    cached, err := h.cacheSvc.GetRecommendation(ctx, req.Query)
    if err == nil && cached != nil {
        c.JSON(http.StatusOK, cached)
        return
    }
    
    // 2. 调用推荐服务
    result, err := h.recommendSvc.Recommend(ctx, req.Query, req.UserID, req.TopK)
    if err != nil {
        c.JSON(http.StatusInternalServerError, gin.H{"error": err.Error()})
        return
    }
    
    // 3. 构造响应
    response := RecommendResponse{
        Status:          "success",
        Query:           req.Query,
        Recommendations: result.Templates,
        Explanation:     result.Explanation,
        ResponseTimeMs:  time.Since(startTime).Milliseconds(),
    }
    
    // 4. 缓存结果
    go h.cacheSvc.CacheRecommendation(context.Background(), req.Query, response)
    
    c.JSON(http.StatusOK, response)
}
```

### 7.2 Golang - 推荐服务

```go
// internal/service/recommend_service.go
package service

import (
    "context"
    "encoding/json"
    "fmt"
    
    "golang.org/x/sync/errgroup"
    "template-recommend/internal/client"
    "template-recommend/internal/model"
)

type RecommendService struct {
    aiClient      *client.AIServiceClient
    vectorSvc     *VectorSearchService
    tagSvc        *TagFilterService
    keywordSvc    *KeywordSearchService
    fusionSvc     *ResultFusionService
}

type RecommendResult struct {
    Templates   []model.Template
    Explanation string
    Intent      *model.Intent
}

func (s *RecommendService) Recommend(
    ctx context.Context, 
    query string, 
    userID string, 
    topK int,
) (*RecommendResult, error) {
    // 1. 调用Python AI服务理解意图
    intent, err := s.aiClient.UnderstandIntent(ctx, query, userID)
    if err != nil {
        return nil, fmt.Errorf("intent understanding failed: %w", err)
    }
    
    // 2. 根据策略并行调用检索工具
    var (
        vectorResults  []model.Template
        tagResults     []model.Template
        keywordResults []model.Template
    )
    
    g, gctx := errgroup.WithContext(ctx)
    
    // 向量检索
    if intent.SearchStrategy == "vector" || intent.SearchStrategy == "hybrid" {
        g.Go(func() error {
            // 先调用Python生成embedding
            embedding, err := s.aiClient.GenerateEmbedding(gctx, query)
            if err != nil {
                return err
            }
            
            // 向量检索
            vectorResults, err = s.vectorSvc.Search(gctx, embedding, topK*2)
            return err
        })
    }
    
    // 标签过滤
    if len(intent.Tags) > 0 {
        g.Go(func() error {
            var err error
            tagResults, err = s.tagSvc.FilterByTags(gctx, intent.Tags, topK*2)
            return err
        })
    }
    
    // 关键词搜索
    if len(intent.Keywords) > 0 {
        g.Go(func() error {
            var err error
            keywordResults, err = s.keywordSvc.Search(gctx, intent.Keywords, topK)
            return err
        })
    }
    
    // 等待所有检索完成
    if err := g.Wait(); err != nil {
        return nil, fmt.Errorf("search failed: %w", err)
    }
    
    // 3. 融合多路召回结果
    fusedResults := s.fusionSvc.Merge(
        vectorResults, 
        tagResults, 
        keywordResults,
        topK,
    )
    
    // 4. 调用Python生成推荐理由
    explanation, err := s.aiClient.GenerateExplanation(ctx, query, fusedResults)
    if err != nil {
        // 生成失败不影响主流程
        explanation = "为您推荐以下模版"
    }
    
    return &RecommendResult{
        Templates:   fusedResults,
        Explanation: explanation,
        Intent:      intent,
    }, nil
}
```

### 7.3 Golang - 向量检索服务

```go
// internal/service/vector_search_service.go
package service

import (
    "context"
    "fmt"
    
    qdrant "github.com/qdrant/go-client/qdrant"
    "template-recommend/internal/model"
    "template-recommend/internal/repository"
)

type VectorSearchService struct {
    qdrantClient *qdrant.Client
    templateRepo *repository.TemplateRepository
    collectionName string
}

func NewVectorSearchService(
    client *qdrant.Client, 
    repo *repository.TemplateRepository,
) *VectorSearchService {
    return &VectorSearchService{
        qdrantClient:   client,
        templateRepo:   repo,
        collectionName: "templates",
    }
}

func (s *VectorSearchService) Search(
    ctx context.Context, 
    embedding []float32, 
    topK int,
) ([]model.Template, error) {
    // 1. 在Qdrant中检索
    searchResult, err := s.qdrantClient.Query(ctx, &qdrant.QueryPoints{
        CollectionName: s.collectionName,
        Query:          qdrant.NewQuery(embedding...),
        Limit:          uint64(topK),
        WithPayload:    qdrant.NewWithPayload(true),
    })
    
    if err != nil {
        return nil, fmt.Errorf("qdrant search failed: %w", err)
    }
    
    // 2. 提取template IDs
    var templateIDs []string
    scoreMap := make(map[string]float32)
    
    for _, point := range searchResult {
        if id, ok := point.Payload["template_id"].(string); ok {
            templateIDs = append(templateIDs, id)
            scoreMap[id] = point.Score
        }
    }
    
    // 3. 从数据库批量获取完整模版信息
    templates, err := s.templateRepo.GetByIDs(ctx, templateIDs)
    if err != nil {
        return nil, fmt.Errorf("get templates failed: %w", err)
    }
    
    // 4. 添加相似度分数
    for i := range templates {
        templates[i].VectorScore = scoreMap[templates[i].TemplateID]
    }
    
    return templates, nil
}

// 批量添加模版到向量库
func (s *VectorSearchService) AddTemplates(
    ctx context.Context, 
    templates []model.Template, 
    embeddings [][]float32,
) error {
    if len(templates) != len(embeddings) {
        return fmt.Errorf("templates and embeddings length mismatch")
    }
    
    var points []*qdrant.PointStruct
    for i, tmpl := range templates {
        points = append(points, &qdrant.PointStruct{
            Id: qdrant.NewIDNum(uint64(tmpl.ID)),
            Vectors: qdrant.NewVectors(embeddings[i]...),
            Payload: qdrant.NewValueMap(map[string]interface{}{
                "template_id": tmpl.TemplateID,
                "name":        tmpl.Name,
                "category":    tmpl.Category,
                "tags":        tmpl.Tags,
            }),
        })
    }
    
    _, err := s.qdrantClient.Upsert(ctx, &qdrant.UpsertPoints{
        CollectionName: s.collectionName,
        Points:         points,
    })
    
    return err
}
```

### 7.4 Golang - 标签过滤服务

```go
// internal/service/tag_filter_service.go
package service

import (
    "context"
    
    "template-recommend/internal/model"
    "template-recommend/internal/repository"
)

type TagFilterService struct {
    templateRepo *repository.TemplateRepository
}

func (s *TagFilterService) FilterByTags(
    ctx context.Context, 
    tags []string, 
    topK int,
) ([]model.Template, error) {
    return s.templateRepo.FilterByTags(ctx, tags, topK)
}
```

```go
// internal/repository/template_repository.go
package repository

import (
    "context"
    
    "gorm.io/gorm"
    "template-recommend/internal/model"
)

type TemplateRepository struct {
    db *gorm.DB
}

func (r *TemplateRepository) FilterByTags(
    ctx context.Context, 
    tags []string, 
    limit int,
) ([]model.Template, error) {
    var templates []model.Template
    
    // PostgreSQL数组重叠查询
    err := r.db.WithContext(ctx).
        Where("tags && ?", tags).  // && 是PostgreSQL数组重叠操作符
        Order("cardinality(tags & ?) DESC", tags).  // 按匹配标签数量排序
        Limit(limit).
        Find(&templates).Error
    
    return templates, err
}

func (r *TemplateRepository) GetByIDs(
    ctx context.Context, 
    ids []string,
) ([]model.Template, error) {
    var templates []model.Template
    err := r.db.WithContext(ctx).
        Where("template_id IN ?", ids).
        Find(&templates).Error
    return templates, err
}
```

### 7.5 Golang - 结果融合服务

```go
// internal/service/result_fusion_service.go
package service

import (
    "sort"
    
    "template-recommend/internal/model"
)

type ResultFusionService struct {
    k float64 // RRF参数
}

func NewResultFusionService() *ResultFusionService {
    return &ResultFusionService{k: 60.0}
}

// RRF (Reciprocal Rank Fusion) 算法融合多路召回
func (s *ResultFusionService) Merge(
    vectorResults []model.Template,
    tagResults []model.Template,
    keywordResults []model.Template,
    topK int,
) []model.Template {
    scores := make(map[string]float64)
    templates := make(map[string]model.Template)
    
    // 向量检索结果打分
    for rank, tmpl := range vectorResults {
        score := 1.0 / (s.k + float64(rank+1))
        scores[tmpl.TemplateID] = scores[tmpl.TemplateID] + score*0.5
        templates[tmpl.TemplateID] = tmpl
    }
    
    // 标签过滤结果打分
    for rank, tmpl := range tagResults {
        score := 1.0 / (s.k + float64(rank+1))
        scores[tmpl.TemplateID] = scores[tmpl.TemplateID] + score*0.3
        templates[tmpl.TemplateID] = tmpl
    }
    
    // 关键词搜索结果打分
    for rank, tmpl := range keywordResults {
        score := 1.0 / (s.k + float64(rank+1))
        scores[tmpl.TemplateID] = scores[tmpl.TemplateID] + score*0.2
        templates[tmpl.TemplateID] = tmpl
    }
    
    // 排序
    type scoredTemplate struct {
        template model.Template
        score    float64
    }
    
    var scored []scoredTemplate
    for id, tmpl := range templates {
        scored = append(scored, scoredTemplate{
            template: tmpl,
            score:    scores[id],
        })
    }
    
    sort.Slice(scored, func(i, j int) bool {
        return scored[i].score > scored[j].score
    })
    
    // 返回Top-K
    var result []model.Template
    for i := 0; i < topK && i < len(scored); i++ {
        result = append(result, scored[i].template)
    }
    
    return result
}
```

### 7.6 Golang - gRPC客户端 (调用Python服务)

```go
// internal/client/ai_service_client.go
package client

import (
    "context"
    "fmt"
    
    "google.golang.org/grpc"
    "google.golang.org/grpc/credentials/insecure"
    
    pb "template-recommend/api/proto"
    "template-recommend/internal/model"
)

type AIServiceClient struct {
    conn   *grpc.ClientConn
    client pb.AIServiceClient
}

func NewAIServiceClient(addr string) (*AIServiceClient, error) {
    conn, err := grpc.Dial(addr, grpc.WithTransportCredentials(insecure.NewCredentials()))
    if err != nil {
        return nil, fmt.Errorf("failed to connect: %w", err)
    }
    
    return &AIServiceClient{
        conn:   conn,
        client: pb.NewAIServiceClient(conn),
    }, nil
}

func (c *AIServiceClient) UnderstandIntent(
    ctx context.Context,
    query string,
    userID string,
) (*model.Intent, error) {
    req := &pb.IntentRequest{
        Query:  query,
        UserId: userID,
    }
    
    resp, err := c.client.UnderstandIntent(ctx, req)
    if err != nil {
        return nil, err
    }
    
    return &model.Intent{
        Intent:         resp.Intent,
        Features:       resp.Features,
        Keywords:       resp.Keywords,
        Tags:           resp.Tags,
        SearchStrategy: resp.SearchStrategy,
    }, nil
}

func (c *AIServiceClient) GenerateEmbedding(
    ctx context.Context,
    text string,
) ([]float32, error) {
    req := &pb.EmbeddingRequest{
        Text: text,
    }
    
    resp, err := c.client.GenerateEmbedding(ctx, req)
    if err != nil {
        return nil, err
    }
    
    return resp.Embedding, nil
}

func (c *AIServiceClient) GenerateExplanation(
    ctx context.Context,
    query string,
    templates []model.Template,
) (string, error) {
    var pbTemplates []*pb.Template
    for _, tmpl := range templates {
        pbTemplates = append(pbTemplates, &pb.Template{
            TemplateId:  tmpl.TemplateID,
            Name:        tmpl.Name,
            Description: tmpl.Description,
            Tags:        tmpl.Tags,
        })
    }
    
    req := &pb.ExplanationRequest{
        Query:     query,
        Templates: pbTemplates,
    }
    
    resp, err := c.client.GenerateExplanation(ctx, req)
    if err != nil {
        return "", err
    }
    
    return resp.Explanation, nil
}

func (c *AIServiceClient) Close() error {
    return c.conn.Close()
}
```

### 7.7 Python - AI服务 (gRPC Server)

```python
# ai_service/server.py
import grpc
from concurrent import futures
import logging
from typing import List

from ai_service.proto import ai_service_pb2
from ai_service.proto import ai_service_pb2_grpc
from ai_service.agent import TemplateAgent
from ai_service.embedding import EmbeddingService

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class AIServicer(ai_service_pb2_grpc.AIServiceServicer):
    def __init__(self):
        self.agent = TemplateAgent()
        self.embedding_service = EmbeddingService()
    
    def UnderstandIntent(self, request, context):
        """理解用户意图"""
        try:
            logger.info(f"Understanding intent for query: {request.query}")
            
            # 调用Agent分析意图
            intent_result = self.agent.understand_intent(
                query=request.query,
                user_id=request.user_id,
                context=list(request.context)
            )
            
            return ai_service_pb2.IntentResponse(
                intent=intent_result['intent'],
                features=intent_result['features'],
                keywords=intent_result['keywords'],
                tags=intent_result['tags'],
                search_strategy=intent_result['search_strategy']
            )
        except Exception as e:
            logger.error(f"Intent understanding failed: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return ai_service_pb2.IntentResponse()
    
    def GenerateEmbedding(self, request, context):
        """生成文本embedding"""
        try:
            embedding = self.embedding_service.encode(request.text)
            
            return ai_service_pb2.EmbeddingResponse(
                embedding=embedding.tolist(),
                dimension=len(embedding)
            )
        except Exception as e:
            logger.error(f"Embedding generation failed: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return ai_service_pb2.EmbeddingResponse()
    
    def GenerateExplanation(self, request, context):
        """生成推荐解释"""
        try:
            templates = [
                {
                    'template_id': t.template_id,
                    'name': t.name,
                    'description': t.description,
                    'tags': list(t.tags)
                }
                for t in request.templates
            ]
            
            explanation = self.agent.generate_explanation(
                query=request.query,
                templates=templates
            )
            
            return ai_service_pb2.ExplanationResponse(
                explanation=explanation,
                reasons=[]  # 可选:返回每个模版的推荐理由
            )
        except Exception as e:
            logger.error(f"Explanation generation failed: {e}")
            context.set_code(grpc.StatusCode.INTERNAL)
            context.set_details(str(e))
            return ai_service_pb2.ExplanationResponse()


def serve():
    server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))
    ai_service_pb2_grpc.add_AIServiceServicer_to_server(
        AIServicer(), server
    )
    server.add_insecure_port('[::]:50051')
    server.start()
    logger.info("AI Service started on port 50051")
    server.wait_for_termination()


if __name__ == '__main__':
    serve()
```

### 7.8 Python - Agent实现

```python
# ai_service/agent.py
from langchain.agents import initialize_agent, AgentType
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from typing import Dict, List
import json


class TemplateAgent:
    def __init__(self):
        self.llm = ChatOpenAI(
            model="gpt-4-turbo-preview",
            temperature=0.3
        )
        self.intent_prompt = self._create_intent_prompt()
        self.explanation_prompt = self._create_explanation_prompt()
    
    def _create_intent_prompt(self):
        return ChatPromptTemplate.from_messages([
            ("system", """你是一个专业的设计模版意图理解助手。
            分析用户的查询并提取以下信息:
            1. intent: 用户意图 (如"生成海报"、"制作名片"等)
            2. features: 关键特征字典 (风格、场景、色调、用途等)
            3. keywords: 关键词列表
            4. tags: 标签列表 (用于精准过滤)
            5. search_strategy: 检索策略 ("vector"/"tag"/"hybrid")
            
            返回JSON格式,不要包含其他文字。"""),
            ("user", "{query}")
        ])
    
    def _create_explanation_prompt(self):
        return ChatPromptTemplate.from_messages([
            ("system", """你是一个专业的设计模版推荐助手。
            基于用户查询和推荐的模版,生成简洁的推荐说明。
            
            格式要求:
            - 开头一句话总结
            - 列出每个模版的推荐理由(20字以内)
            - 语气友好专业"""),
            ("user", """
            用户查询: {query}
            
            推荐模版:
            {templates}
            
            请生成推荐说明:""")
        ])
    
    def understand_intent(
        self, 
        query: str, 
        user_id: str = None,
        context: List[str] = None
    ) -> Dict:
        """理解用户意图"""
        try:
            # 调用LLM分析意图
            chain = self.intent_prompt | self.llm
            response = chain.invoke({"query": query})
            
            # 解析JSON响应
            result = json.loads(response.content)
            
            # 确保有默认值
            return {
                'intent': result.get('intent', '模版推荐'),
                'features': result.get('features', {}),
                'keywords': result.get('keywords', []),
                'tags': result.get('tags', []),
                'search_strategy': result.get('search_strategy', 'hybrid')
            }
        except Exception as e:
            # 降级处理:返回基础意图
            return {
                'intent': '模版推荐',
                'features': {},
                'keywords': query.split(),
                'tags': [],
                'search_strategy': 'vector'
            }
    
    def generate_explanation(
        self, 
        query: str, 
        templates: List[Dict]
    ) -> str:
        """生成推荐解释"""
        try:
            # 格式化模版信息
            templates_text = "\n".join([
                f"{i+1}. {t['name']} - {t['description']}"
                for i, t in enumerate(templates[:5])
            ])
            
            chain = self.explanation_prompt | self.llm
            response = chain.invoke({
                "query": query,
                "templates": templates_text
            })
            
            return response.content
        except Exception as e:
            # 降级处理
            return f"为您推荐以下{len(templates)}个模版"
```

### 7.9 Python - Embedding服务

```python
# ai_service/embedding.py
import openai
import numpy as np
from typing import List, Union
from sentence_transformers import SentenceTransformer
import torch


class EmbeddingService:
    def __init__(self, use_local_model: bool = False):
        self.use_local_model = use_local_model
        
        if use_local_model:
            # 使用本地BGE模型
            self.model = SentenceTransformer('BAAI/bge-large-zh-v1.5')
            self.dimension = 1024
        else:
            # 使用OpenAI API
            self.model_name = "text-embedding-3-small"
            self.dimension = 1536
    
    def encode(self, text: Union[str, List[str]]) -> np.ndarray:
        """编码文本为向量"""
        if self.use_local_model:
            return self._encode_local(text)
        else:
            return self._encode_openai(text)
    
    def _encode_local(self, text: Union[str, List[str]]) -> np.ndarray:
        """使用本地模型编码"""
        embeddings = self.model.encode(
            text,
            normalize_embeddings=True,
            show_progress_bar=False
        )
        return embeddings
    
    def _encode_openai(self, text: Union[str, List[str]]) -> np.ndarray:
        """使用OpenAI API编码"""
        if isinstance(text, str):
            text = [text]
        
        response = openai.Embedding.create(
            model=self.model_name,
            input=text
        )
        
        embeddings = [item['embedding'] for item in response['data']]
        return np.array(embeddings[0] if len(text) == 1 else embeddings)
    
    def batch_encode(self, texts: List[str], batch_size: int = 32) -> List[np.ndarray]:
        """批量编码"""
        embeddings = []
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            batch_embeddings = self.encode(batch)
            embeddings.extend(batch_embeddings)
        return embeddings
```

---

## 8. 性能优化策略

### 8.1 Golang缓存层

```go
// internal/service/cache_service.go
package service

import (
    "context"
    "crypto/md5"
    "encoding/hex"
    "encoding/json"
    "fmt"
    "time"
    
    "github.com/go-redis/redis/v8"
)

type CacheService struct {
    client *redis.Client
    ttl    time.Duration
}

func NewCacheService(client *redis.Client) *CacheService {
    return &CacheService{
        client: client,
        ttl:    1 * time.Hour,
    }
}

func (s *CacheService) GetRecommendation(
    ctx context.Context, 
    query string,
) (interface{}, error) {
    key := s.generateKey(query)
    
    val, err := s.client.Get(ctx, key).Result()
    if err == redis.Nil {
        return nil, nil
    }
    if err != nil {
        return nil, err
    }
    
    var result interface{}
    if err := json.Unmarshal([]byte(val), &result); err != nil {
        return nil, err
    }
    
    return result, nil
}

func (s *CacheService) CacheRecommendation(
    ctx context.Context,
    query string,
    result interface{},
) error {
    key := s.generateKey(query)
    
    data, err := json.Marshal(result)
    if err != nil {
        return err
    }
    
    return s.client.Set(ctx, key, data, s.ttl).Err()
}

func (s *CacheService) generateKey(query string) string {
    hash := md5.Sum([]byte(query))
    return fmt.Sprintf("recommend:%s", hex.EncodeToString(hash[:]))
}
```

### 8.2 连接池配置

```go
// internal/config/database.go
package config

import (
    "time"
    
    "gorm.io/driver/postgres"
    "gorm.io/gorm"
)

func InitDB(dsn string) (*gorm.DB, error) {
    db, err := gorm.Open(postgres.Open(dsn), &gorm.Config{})
    if err != nil {
        return nil, err
    }
    
    sqlDB, err := db.DB()
    if err != nil {
        return nil, err
    }
    
    // 连接池配置
    sqlDB.SetMaxIdleConns(10)
    sqlDB.SetMaxOpenConns(100)
    sqlDB.SetConnMaxLifetime(time.Hour)
    
    return db, nil
}
```

### 8.3 异步任务处理 (RabbitMQ)

```go
// internal/queue/publisher.go
package queue

import (
    "encoding/json"
    "fmt"
    
    "github.com/streadway/amqp"
)

type Publisher struct {
    conn    *amqp.Connection
    channel *amqp.Channel
}

func NewPublisher(url string) (*Publisher, error) {
    conn, err := amqp.Dial(url)
    if err != nil {
        return nil, err
    }
    
    ch, err := conn.Channel()
    if err != nil {
        return nil, err
    }
    
    return &Publisher{
        conn:    conn,
        channel: ch,
    }, nil
}

func (p *Publisher) PublishEmbeddingTask(templateID string, description string) error {
    task := map[string]string{
        "template_id": templateID,
        "description": description,
    }
    
    body, err := json.Marshal(task)
    if err != nil {
        return err
    }
    
    return p.channel.Publish(
        "",                    // exchange
        "embedding_tasks",     // routing key
        false,                 // mandatory
        false,                 // immediate
        amqp.Publishing{
            ContentType: "application/json",
            Body:        body,
        },
    )
}
```

```python
# ai_service/worker.py
import pika
import json
from ai_service.embedding import EmbeddingService
from ai_service.vector_store import VectorStore


class EmbeddingWorker:
    def __init__(self):
        self.embedding_service = EmbeddingService()
        self.vector_store = VectorStore()
        
        connection = pika.BlockingConnection(
            pika.ConnectionParameters('localhost')
        )
        self.channel = connection.channel()
        self.channel.queue_declare(queue='embedding_tasks')
    
    def callback(self, ch, method, properties, body):
        """处理embedding任务"""
        try:
            task = json.loads(body)
            template_id = task['template_id']
            description = task['description']
            
            # 生成embedding
            embedding = self.embedding_service.encode(description)
            
            # 存储到向量库
            self.vector_store.upsert(template_id, embedding)
            
            ch.basic_ack(delivery_tag=method.delivery_tag)
        except Exception as e:
            print(f"Task failed: {e}")
            ch.basic_nack(delivery_tag=method.delivery_tag)
    
    def start(self):
        self.channel.basic_consume(
            queue='embedding_tasks',
            on_message_callback=self.callback
        )
        print('Embedding worker started')
        self.channel.start_consuming()


if __name__ == '__main__':
    worker = EmbeddingWorker()
    worker.start()
```

---

## 9. 部署方案

### 9.1 Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  # Golang API服务
  api-gateway:
    build:
      context: ./go-service
      dockerfile: Dockerfile
    ports:
      - "8080:8080"
    environment:
      - DB_HOST=postgres
      - REDIS_HOST=redis
      - AI_SERVICE_ADDR=ai-service:50051
      - QDRANT_HOST=qdrant
    depends_on:
      - postgres
      - redis
      - qdrant
      - ai-service
    restart: always
  
  # Python AI服务
  ai-service:
    build:
      context: ./python-service
      dockerfile: Dockerfile
    ports:
      - "50051:50051"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    restart: always
  
  # Python Embedding Worker
  embedding-worker:
    build:
      context: ./python-service
      dockerfile: Dockerfile.worker
    environment:
      - RABBITMQ_HOST=rabbitmq
      - QDRANT_HOST=qdrant
    depends_on:
      - rabbitmq
      - qdrant
    restart: always
  
  # PostgreSQL
  postgres:
    image: postgres:15
    environment:
      POSTGRES_DB: templates
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
  
  # Redis
  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
  
  # Qdrant向量数据库
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage
  
  # RabbitMQ
  rabbitmq:
    image: rabbitmq:3-management
    ports:
      - "5672:5672"
      - "15672:15672"
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
  
  # MinIO对象存储
  minio:
    image: minio/minio
    ports:
      - "9000:9000"
      - "9001:9001"
    environment:
      MINIO_ROOT_USER: admin
      MINIO_ROOT_PASSWORD: password123
    command: server /data --console-address ":9001"
    volumes:
      - minio_data:/data

volumes:
  postgres_data:
  redis_data:
  qdrant_data:
  rabbitmq_data:
  minio_data:
```

### 9.2 Golang Dockerfile

```dockerfile
# go-service/Dockerfile
FROM golang:1.21-alpine AS builder

WORKDIR /app

# 依赖缓存
COPY go.mod go.sum ./
RUN go mod download

# 编译
COPY . .
RUN CGO_ENABLED=0 GOOS=linux go build -o /api-server ./cmd/api

# 运行镜像
FROM alpine:latest

RUN apk --no-cache add ca-certificates

WORKDIR /root/

COPY --from=builder /api-server .

EXPOSE 8080

CMD ["./api-server"]
```

### 9.3 Python Dockerfile

```dockerfile
# python-service/Dockerfile
FROM python:3.11-slim

WORKDIR /app

# 安装依赖
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# 复制代码
COPY . .

# 生成gRPC代码
RUN python -m grpc_tools.protoc \
    -I./proto \
    --python_out=./ai_service/proto \
    --grpc_python_out=./ai_service/proto \
    ./proto/ai_service.proto

EXPOSE 50051

CMD ["python", "-m", "ai_service.server"]
```

### 9.4 Kubernetes部署

```yaml
# k8s/api-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: api-gateway
spec:
  replicas: 3
  selector:
    matchLabels:
      app: api-gateway
  template:
    metadata:
      labels:
        app: api-gateway
    spec:
      containers:
      - name: api
        image: template-recommend/api:latest
        ports:
        - containerPort: 8080
        env:
        - name: DB_HOST
          value: postgres-service
        - name: REDIS_HOST
          value: redis-service
        - name: AI_SERVICE_ADDR
          value: ai-service:50051
        resources:
          requests:
            memory: "256Mi"
            cpu: "250m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5
---
apiVersion: v1
kind: Service
metadata:
  name: api-gateway-service
spec:
  selector:
    app: api-gateway
  ports:
  - protocol: TCP
    port: 80
    targetPort: 8080
  type: LoadBalancer
```

```yaml
# k8s/ai-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: ai-service
spec:
  replicas: 2
  selector:
    matchLabels:
      app: ai-service
  template:
    metadata:
      labels:
        app: ai-service
    spec:
      containers:
      - name: ai
        image: template-recommend/ai:latest
        ports:
        - containerPort: 50051
        env:
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: api-secrets
              key: openai-key
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "4Gi"
            cpu: "2000m"
---
apiVersion: v1
kind: Service
metadata:
  name: ai-service
spec:
  selector:
    app: ai-service
  ports:
  - protocol: TCP
    port: 50051
    targetPort: 50051
```

---

## 10. 监控与可观测性

### 10.1 Prometheus指标 (Golang)

```go
// internal/metrics/metrics.go
package metrics

import (
    "github.com/prometheus/client_golang/prometheus"
    "github.com/prometheus/client_golang/prometheus/promauto"
)

var (
    // 请求计数
    RequestCount = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "api_requests_total",
            Help: "Total number of API requests",
        },
        []string{"method", "endpoint", "status"},
    )
    
    // 响应时间
    ResponseTime = promauto.NewHistogramVec(
        prometheus.HistogramOpts{
            Name:    "api_response_time_seconds",
            Help:    "API response time in seconds",
            Buckets: prometheus.DefBuckets,
        },
        []string{"method", "endpoint"},
    )
    
    // AI服务调用
    AIServiceCalls = promauto.NewCounterVec(
        prometheus.CounterOpts{
            Name: "ai_service_calls_total",
            Help: "Total number of AI service calls",
        },
        []string{"method", "status"},
    )
    
    // 缓存命中率
    CacheHits = promauto.NewCounter(
        prometheus.CounterOpts{
            Name: "cache_hits_total",
            Help: "Total number of cache hits",
        },
    )
    
    CacheMisses = promauto.NewCounter(
        prometheus.CounterOpts{
            Name: "cache_misses_total",
            Help: "Total number of cache misses",
        },
    )
)
```

```go
// internal/middleware/metrics.go
package middleware

import (
    "time"
    
    "github.com/gin-gonic/gin"
    "template-recommend/internal/metrics"
)

func MetricsMiddleware() gin.HandlerFunc {
    return func(c *gin.Context) {
        start := time.Now()
        
        c.Next()
        
        duration := time.Since(start).Seconds()
        status := c.Writer.Status()
        
        metrics.RequestCount.WithLabelValues(
            c.Request.Method,
            c.FullPath(),
            fmt.Sprint(status),
        ).Inc()
        
        metrics.ResponseTime.WithLabelValues(
            c.Request.Method,
            c.FullPath(),
        ).Observe(duration)
    }
}
```

### 10.2 结构化日志 (Golang)

```go
// internal/logger/logger.go
package logger

import (
    "go.uber.org/zap"
    "go.uber.org/zap/zapcore"
)

var Log *zap.Logger

func Init() {
    config := zap.NewProductionConfig()
    config.EncoderConfig.TimeKey = "timestamp"
    config.EncoderConfig.EncodeTime = zapcore.ISO8601TimeEncoder
    
    var err error
    Log, err = config.Build()
    if err != nil {
        panic(err)
    }
}

func Info(msg string, fields ...zap.Field) {
    Log.Info(msg, fields...)
}

func Error(msg string, fields ...zap.Field) {
    Log.Error(msg, fields...)
}

func Warn(msg string, fields ...zap.Field) {
    Log.Warn(msg, fields...)
}
```

---

## 11. 成本估算与优化

### 11.1 成本分析 (日均1万次推荐)

| 项目 | 方案 | 月成本(USD) |
|------|------|------------|
| **LLM调用** | OpenAI GPT-4 Turbo | $5,000 |
| **Embedding** | OpenAI API | $13 |
| **计算资源** | 3台2C4G服务器 | $150 |
| **数据库** | PostgreSQL RDS | $50 |
| **向量库** | Qdrant自建 | $30 |
| **缓存** | Redis | $20 |
| **对象存储** | MinIO自建 | $10 |
| **总计** | - | **$5,273** |

### 11.2 优化后成本

| 优化措施 | 节省成本 | 优化后成本 |
|---------|---------|-----------|
| 智能缓存(50%命中率) | -$2,500 | $2,500 |
| 本地BGE模型 | -$13 | $0 |
| 批量调用 | -$500 | $2,000 |
| **总计** | **-$3,013** | **$2,260/月** |

---

## 12. 总结

### 12.1 Golang + Python混合架构优势

✅ **性能最优**: Golang处理高并发API请求,性能是Python的10倍+
✅ **专业分工**: Python专注AI能力,Golang专注业务逻辑
✅ **成本可控**: 本地模型 + 智能缓存大幅降低LLM成本
✅ **易于扩展**: 服务解耦,可独立扩展
✅ **运维友好**: Golang单二进制部署,Python容器化部署

### 12.2 实施路线图

**第一阶段 (1-2个月) - MVP**
- ✅ Golang API Gateway + 基础CRUD
- ✅ Python AI Service (gRPC)
- ✅ PostgreSQL + Qdrant
- ✅ 基础向量检索

**第二阶段 (2-3个月) - 优化**
- ✅ 添加标签过滤、混合检索
- ✅ Redis缓存层
- ✅ RabbitMQ异步任务
- ✅ 监控告警系统

**第三阶段 (3-6个月) - 规模化**
- ✅ 本地Embedding模型
- ✅ 个性化推荐
- ✅ 多轮对话
- ✅ Kubernetes部署

---

**文档版本**: v2.0 (Golang + Python混合架构)
**最后更新**: 2026年2月4日
**技术架构**: Golang (业务) + Python (AI)
